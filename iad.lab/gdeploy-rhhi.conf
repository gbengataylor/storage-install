[hosts]
172.16.2.21
172.16.2.22
172.16.2.23

[disktype]
jbod

[service1]
action=enable
service=chronyd

[service2]
action=restart
service=chronyd

[shell2]
action=execute
command=vdsm-tool configure --force

[script3]
action=execute
file=/usr/share/gdeploy/scripts/blacklist_all_disks.sh
ignore_script_errors=no

[selinux]
yes

[service3]
action=restart
service=glusterd
slice_setup=yes

[firewalld]
action=add
ports=111/tcp,2049/tcp,54321/tcp,5900/tcp,5900-6923/tcp,5666/tcp,16514/tcp,54322/tcp
services=glusterfs

[script2]
action=execute
file=/usr/share/gdeploy/scripts/disable-gluster-hooks.sh

[shell3]
action=execute
command=usermod -a -G gluster qemu

[volume1]  #engine (1x3) /bricks/nvme0n1p1/engine
action=create
volname=engine
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,features.shard-block-size
value=virt,36,36,30,on,off,enable,64MB
ignore_volume_errors=no
brick_dirs=172.16.2.21:/bricks/nvme0n1p1/engine,172.16.2.22:/bricks/nvme0n1p1/engine,172.16.2.23:/bricks/nvme0n1p1/engine

[volume2]  #iso (1x3) /bricks/nvme0n1p2/iso
action=create
volname=iso
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,features.shard-block-size
value=virt,36,36,30,on,off,enable,64MB
ignore_volume_errors=no
brick_dirs=172.16.2.21:/bricks/nvme0n1p2/iso,172.16.2.22:/bricks/nvme0n1p2/iso,172.16.2.23:/bricks/nvme0n1p2/iso

[volume3]  #export (1x3) /bricks/nvme0n1p3/export
action=create
volname=export
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,features.shard-block-size
value=virt,36,36,30,on,off,enable,64MB
ignore_volume_errors=no
brick_dirs=172.16.2.21:/bricks/nvme0n1p3/export,172.16.2.22:/bricks/nvme0n1p3/export,172.16.2.23:/bricks/nvme0n1p3/export

[volume4]  #vmstore1 (1x3) /bricks/nvme0n1p4/vmstore1
action=create
volname=vmstore1
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,features.shard-block-size
value=virt,36,36,30,on,off,enable,64MB
ignore_volume_errors=no
brick_dirs=172.16.2.21:/bricks/nvme0n1p4/vmstore1,172.16.2.22:/bricks/nvme0n1p4/vmstore1,172.16.2.23:/bricks/nvme0n1p4/vmstore1

[volume5]  #vmstore2 (1x3) /bricks/nvme1n1p1/vmstore2
action=create
volname=vmstore2
transport=tcp
replica=yes
replica_count=3
key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,features.shard-block-size
value=virt,36,36,30,on,off,enable,64MB
ignore_volume_errors=no
brick_dirs=172.16.2.21:/bricks/nvme1n1p1/vmstore2,172.16.2.22:/bricks/nvme1n1p1/vmstore2,172.16.2.23:/bricks/nvme1n1p1/vmstore2

## Do this sperately, so we can set DISKTYPE, DISKCOUNT, & STRIPESIZE
#[volume6]  #vmstore3 (1x3) /bricks/sda1/vmstore3
#action=create
#volname=vmstore3
#transport=tcp
#replica=yes
#replica_count=3
#key=group,storage.owner-uid,storage.owner-gid,network.ping-timeout,performance.strict-o-direct,network.remote-dio,cluster.granular-entry-heal,features.shard-block-size
#value=virt,36,36,30,on,off,enable,64MB
#ignore_volume_errors=no
#brick_dirs=172.16.2.21:/bricks/sda1/vmstore3,172.16.2.22:/bricks/sda1/vmstore3,172.16.2.23:/bricks/sda1/vmstore3
